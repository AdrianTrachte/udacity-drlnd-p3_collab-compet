{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program is beeing solved with two agents utilising the DDPG algorithm and a shared replay buffer.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: progressbar in c:\\users\\adria\\anaconda3\\lib\\site-packages (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# import environment\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# import general stuff\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "\n",
    "# import specific / own stuff\n",
    "import utils as ut\n",
    "from ddpg_agent import Agent\n",
    "from ddpg_agent import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.00, Length of Episode is 15 steps\n",
      "Score (max over agents) from episode 2: 0.00, Length of Episode is 14 steps\n",
      "Score (max over agents) from episode 3: 0.00, Length of Episode is 14 steps\n",
      "Score (max over agents) from episode 4: 0.00, Length of Episode is 14 steps\n",
      "Score (max over agents) from episode 5: 0.00, Length of Episode is 14 steps\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    lenEpisode = 0\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        lenEpisode += 1\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {:.2f}, Length of Episode is {} steps'.format(i, np.max(scores), lenEpisode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " get some additional information about the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last set of actions: [[ 0.65871981  1.        ]\n",
      " [-0.90748659  0.55724271]]\n",
      "Size of actions: 4\n",
      "Last rewards: [0.0, -0.009999999776482582]\n",
      "Last states: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.14030886 -1.5\n",
      "  -0.          0.         -7.11741829  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.9574213  -1.5\n",
      "   0.          0.          7.11741829  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('Last set of actions: {}'.format(actions))\n",
    "print('Size of actions: {}'.format(actions.size))\n",
    "print('Last rewards: {}'.format(rewards))\n",
    "print('Last states: {}'.format(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Let's start training!\n",
    "\n",
    "When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define Multi-Agent DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma_ddpg(n_episodes=100, max_t=100, beta_start=1.0, beta_end=0.01, beta_episodeEnd=80):\n",
    "    \"\"\"Deep Deterministic Policy Gradient for multi agent setup.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps\n",
    "        beta_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        beta_end (float): minimum value of epsilon\n",
    "        beta_episodeEnd (float): number of episodes for which eps_end is reached\n",
    "    \"\"\"\n",
    "    solved = False\n",
    "    scores = []                                # list containing scores from each episode\n",
    "    actions = [0 for i in range(num_agents)]   # list containing actions for all agents\n",
    "    scores_window = deque(maxlen=100)          # last 100 scores\n",
    "    steps_window = deque(maxlen=100)           # last 100 steps per episode\n",
    "    beta = beta_start                          # initialize beta, that is factor for adding noise\n",
    "    beta_decay_linear = (beta_start-beta_end)/beta_episodeEnd # compute linear beta decay rate\n",
    "    best_averagescore = 0                      # keep track of best average score to save if desired\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset unity enviroment\n",
    "        state = env_info.vector_observations               # get the current state \n",
    "        score = np.zeros(num_agents)                       # initialize the score (for each agent)\n",
    "        steps = 0                                            \n",
    "        for t in range(max_t):\n",
    "            # choose action and perform it\n",
    "            for i in range(num_agents):\n",
    "                actions[i] = agents[i].act(state[i], beta)\n",
    "            env_info = env.step(actions)[brain_name]       # send the actions to the environment\n",
    "            \n",
    "            # send env data to agent and initiate learning\n",
    "            next_state = env_info.vector_observations      # get the next state\n",
    "            reward = env_info.rewards                      # get the reward\n",
    "            done = env_info.local_done                     # see if episode has finished\n",
    "            \n",
    "            for i in range(num_agents):\n",
    "                agents[i].step(state[i], actions[i], reward[i], next_state[i], done[i])\n",
    "            \n",
    "            # do some tracking on the rewards\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            steps += 1\n",
    "            if np.any(done):\n",
    "                break \n",
    "        scores_window.append(np.max(score))       # save most recent score\n",
    "        steps_window.append(steps)                # save number of steps\n",
    "        scores.append(np.max(score))              # save most recent score\n",
    "        beta = max(beta_end, beta_start - beta_decay_linear*i_episode) # decrease beta\n",
    "        if 2*np.mean(steps_window) > max_t:\n",
    "            max_t = 2*int(np.mean(steps_window)) # increase maximum number of steps\n",
    "        \n",
    "        # give some information about current progress \n",
    "        ut.print_info(i_episode, scores_window, steps_window)\n",
    "        if i_episode % 100 == 0:\n",
    "            # update progress widget bar\n",
    "            timer.update(i_episode)\n",
    "            ut.print_info(i_episode, scores_window, steps_window)\n",
    "            if np.mean(scores_window) > best_averagescore:\n",
    "                best_averagescore = np.mean(scores_window)\n",
    "                ut.save_agentcheckpoint(state_size, action_size, agents) # save agent networks\n",
    "        if np.mean(scores_window)>=0.5 and is not solved:\n",
    "            # yeaaah, we did it!\n",
    "            solved = True\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            # break   # uncomment to keep on going\n",
    "    # finish timer\n",
    "    timer.finish()    \n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the agents and replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio Buffer/Batch: 976.562500\n",
      "Estimate of episodes until learning is initiated: 17.066667\n"
     ]
    }
   ],
   "source": [
    "# parameters for ReplayBuffer\n",
    "BUFFER_SIZE = int(5e5) # replay buffer size\n",
    "BATCH_SIZE = 512       # minibatch size for learning\n",
    "print('Ratio Buffer/Batch: {:.2f}'.format(BUFFER_SIZE/BATCH_SIZE))\n",
    "print('Estimate of episodes until learning is initiated: {:.2f}'.format(BATCH_SIZE/30))\n",
    "\n",
    "# create one common replay memory\n",
    "memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, 321)\n",
    "\n",
    "# create and load agents (optional)\n",
    "agents = ut.create_agents(state_size, action_size, num_agents, memory)\n",
    "# agents = ut.load_agents(state_size, action_size, num_agents, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99\t Score Min/Mean/Max: 0.00 / \u001b[1m0.01\u001b[0m / 0.10 \t Steps Min/Mean/Max: 14 / \u001b[1m15.2 \u001b[0m/ 32"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   2% |#                                          | ETA:  0:44:31\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 199\t Score Min/Mean/Max: 0.00 / \u001b[1m0.01\u001b[0m / 0.10 \t Steps Min/Mean/Max: 13 / \u001b[1m16.1 \u001b[0m/ 50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   5% |##                                         | ETA:  0:49:12\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 299\t Score Min/Mean/Max: 0.00 / \u001b[1m0.00\u001b[0m / 0.10 \t Steps Min/Mean/Max: 14 / \u001b[1m14.5 \u001b[0m/ 31"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |###                                        | ETA:  0:48:21\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 399\t Score Min/Mean/Max: 0.00 / \u001b[1m0.00\u001b[0m / 0.09 \t Steps Min/Mean/Max: 14 / \u001b[1m14.4 \u001b[0m/ 32"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |####                                       | ETA:  0:47:17\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 499\t Score Min/Mean/Max: 0.00 / \u001b[1m0.00\u001b[0m / 0.09 \t Steps Min/Mean/Max: 14 / \u001b[1m14.4 \u001b[0m/ 29"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                      | ETA:  0:45:51\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 599\t Score Min/Mean/Max: 0.00 / \u001b[1m0.00\u001b[0m / 0.00 \t Steps Min/Mean/Max: 13 / \u001b[1m14.2 \u001b[0m/ 15"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  15% |######                                     | ETA:  0:44:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 699\t Score Min/Mean/Max: 0.00 / \u001b[1m0.00\u001b[0m / 0.00 \t Steps Min/Mean/Max: 14 / \u001b[1m14.5 \u001b[0m/ 31"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  17% |#######                                    | ETA:  0:43:12\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 799\t Score Min/Mean/Max: 0.00 / \u001b[1m0.00\u001b[0m / 0.10 \t Steps Min/Mean/Max: 14 / \u001b[1m14.8 \u001b[0m/ 32"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                   | ETA:  0:42:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 899\t Score Min/Mean/Max: 0.00 / \u001b[1m0.01\u001b[0m / 0.10 \t Steps Min/Mean/Max: 13 / \u001b[1m16.7 \u001b[0m/ 37"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  22% |#########                                  | ETA:  0:41:29\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 999\t Score Min/Mean/Max: 0.00 / \u001b[1m0.03\u001b[0m / 0.10 \t Steps Min/Mean/Max: 13 / \u001b[1m18.7 \u001b[0m/ 36"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  25% |##########                                 | ETA:  0:41:16\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1099\t Score Min/Mean/Max: 0.00 / \u001b[1m0.02\u001b[0m / 0.10 \t Steps Min/Mean/Max: 13 / \u001b[1m17.6 \u001b[0m/ 40"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  27% |###########                                | ETA:  0:40:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1199\t Score Min/Mean/Max: 0.00 / \u001b[1m0.03\u001b[0m / 0.20 \t Steps Min/Mean/Max: 13 / \u001b[1m20.3 \u001b[0m/ 72"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  30% |############                               | ETA:  0:40:16\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1299\t Score Min/Mean/Max: 0.00 / \u001b[1m0.04\u001b[0m / 0.20 \t Steps Min/Mean/Max: 14 / \u001b[1m23.2 \u001b[0m/ 71"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  32% |#############                              | ETA:  0:40:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1399\t Score Min/Mean/Max: 0.00 / \u001b[1m0.06\u001b[0m / 0.20 \t Steps Min/Mean/Max: 13 / \u001b[1m25.6 \u001b[0m/ 80"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  35% |###############                            | ETA:  0:40:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1499\t Score Min/Mean/Max: 0.00 / \u001b[1m0.10\u001b[0m / 0.30 \t Steps Min/Mean/Max: 13 / \u001b[1m39.8 \u001b[0m/ 131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  37% |################                           | ETA:  0:42:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1599\t Score Min/Mean/Max: 0.00 / \u001b[1m0.12\u001b[0m / 0.40 \t Steps Min/Mean/Max: 13 / \u001b[1m48.2 \u001b[0m/ 164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |#################                          | ETA:  0:45:38\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1699\t Score Min/Mean/Max: 0.00 / \u001b[1m0.19\u001b[0m / 0.80 \t Steps Min/Mean/Max: 13 / \u001b[1m74.1 \u001b[0m/ 318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  42% |##################                         | ETA:  0:51:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1799\t Score Min/Mean/Max: 0.00 / \u001b[1m0.24\u001b[0m / 1.10 \t Steps Min/Mean/Max: 13 / \u001b[1m98.6 \u001b[0m/ 436"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  45% |###################                        | ETA:  0:57:39\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1870\t Score Min/Mean/Max: 0.00 / \u001b[1m0.50\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m201.9 \u001b[0m/ 1001\n",
      "Environment solved in 1870 episodes!\tAverage Score: 0.50\n",
      "Episode 1871\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m204.6 \u001b[0m/ 1001\n",
      "Environment solved in 1871 episodes!\tAverage Score: 0.51\n",
      "Episode 1872\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m204.4 \u001b[0m/ 1001\n",
      "Environment solved in 1872 episodes!\tAverage Score: 0.51\n",
      "Episode 1873\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m206.5 \u001b[0m/ 1001\n",
      "Environment solved in 1873 episodes!\tAverage Score: 0.51\n",
      "Episode 1874\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m206.7 \u001b[0m/ 1001\n",
      "Environment solved in 1874 episodes!\tAverage Score: 0.52\n",
      "Episode 1875\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m205.8 \u001b[0m/ 1001\n",
      "Environment solved in 1875 episodes!\tAverage Score: 0.51\n",
      "Episode 1876\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m208.6 \u001b[0m/ 1001\n",
      "Environment solved in 1876 episodes!\tAverage Score: 0.52\n",
      "Episode 1877\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m208.4 \u001b[0m/ 1001\n",
      "Environment solved in 1877 episodes!\tAverage Score: 0.52\n",
      "Episode 1878\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m208.7 \u001b[0m/ 1001\n",
      "Environment solved in 1878 episodes!\tAverage Score: 0.52\n",
      "Episode 1879\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m207.9 \u001b[0m/ 1001\n",
      "Environment solved in 1879 episodes!\tAverage Score: 0.52\n",
      "Episode 1880\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m211.4 \u001b[0m/ 1001\n",
      "Environment solved in 1880 episodes!\tAverage Score: 0.53\n",
      "Episode 1881\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m212.7 \u001b[0m/ 1001\n",
      "Environment solved in 1881 episodes!\tAverage Score: 0.53\n",
      "Episode 1882\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m213.0 \u001b[0m/ 1001\n",
      "Environment solved in 1882 episodes!\tAverage Score: 0.53\n",
      "Episode 1883\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m213.4 \u001b[0m/ 1001\n",
      "Environment solved in 1883 episodes!\tAverage Score: 0.53\n",
      "Episode 1884\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m212.7 \u001b[0m/ 1001\n",
      "Environment solved in 1884 episodes!\tAverage Score: 0.53\n",
      "Episode 1885\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m212.0 \u001b[0m/ 1001\n",
      "Environment solved in 1885 episodes!\tAverage Score: 0.53\n",
      "Episode 1886\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m211.7 \u001b[0m/ 1001\n",
      "Environment solved in 1886 episodes!\tAverage Score: 0.53\n",
      "Episode 1887\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m211.4 \u001b[0m/ 1001\n",
      "Environment solved in 1887 episodes!\tAverage Score: 0.53\n",
      "Episode 1888\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m216.5 \u001b[0m/ 1001\n",
      "Environment solved in 1888 episodes!\tAverage Score: 0.54\n",
      "Episode 1889\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m215.5 \u001b[0m/ 1001\n",
      "Environment solved in 1889 episodes!\tAverage Score: 0.54\n",
      "Episode 1890\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m217.6 \u001b[0m/ 1001\n",
      "Environment solved in 1890 episodes!\tAverage Score: 0.55\n",
      "Episode 1891\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m216.9 \u001b[0m/ 1001\n",
      "Environment solved in 1891 episodes!\tAverage Score: 0.55\n",
      "Episode 1892\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m218.9 \u001b[0m/ 1001\n",
      "Environment solved in 1892 episodes!\tAverage Score: 0.55\n",
      "Episode 1893\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m218.8 \u001b[0m/ 1001\n",
      "Environment solved in 1893 episodes!\tAverage Score: 0.55\n",
      "Episode 1894\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m218.4 \u001b[0m/ 1001\n",
      "Environment solved in 1894 episodes!\tAverage Score: 0.55\n",
      "Episode 1895\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m218.8 \u001b[0m/ 1001\n",
      "Environment solved in 1895 episodes!\tAverage Score: 0.55\n",
      "Episode 1896\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m216.3 \u001b[0m/ 1001\n",
      "Environment solved in 1896 episodes!\tAverage Score: 0.55\n",
      "Episode 1897\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m216.9 \u001b[0m/ 1001\n",
      "Environment solved in 1897 episodes!\tAverage Score: 0.55\n",
      "Episode 1898\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m215.8 \u001b[0m/ 1001\n",
      "Environment solved in 1898 episodes!\tAverage Score: 0.54\n",
      "Episode 1899\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m218.8 \u001b[0m/ 1001\n",
      "Environment solved in 1899 episodes!\tAverage Score: 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  47% |####################                       | ETA:  1:15:27\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m217.5 \u001b[0m/ 1001\n",
      "Environment solved in 1900 episodes!\tAverage Score: 0.55\n",
      "Episode 1901\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m216.6 \u001b[0m/ 1001\n",
      "Environment solved in 1901 episodes!\tAverage Score: 0.55\n",
      "Episode 1902\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m214.6 \u001b[0m/ 1001\n",
      "Environment solved in 1902 episodes!\tAverage Score: 0.54\n",
      "Episode 1903\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m216.6 \u001b[0m/ 1001\n",
      "Environment solved in 1903 episodes!\tAverage Score: 0.55\n",
      "Episode 1904\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m214.9 \u001b[0m/ 1001\n",
      "Environment solved in 1904 episodes!\tAverage Score: 0.54\n",
      "Episode 1905\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m215.9 \u001b[0m/ 1001\n",
      "Environment solved in 1905 episodes!\tAverage Score: 0.55\n",
      "Episode 1906\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m215.7 \u001b[0m/ 1001\n",
      "Environment solved in 1906 episodes!\tAverage Score: 0.54\n",
      "Episode 1907\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m217.0 \u001b[0m/ 1001\n",
      "Environment solved in 1907 episodes!\tAverage Score: 0.55\n",
      "Episode 1908\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m217.5 \u001b[0m/ 1001\n",
      "Environment solved in 1908 episodes!\tAverage Score: 0.55\n",
      "Episode 1909\t Score Min/Mean/Max: 0.00 / \u001b[1m0.56\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m219.2 \u001b[0m/ 1001\n",
      "Environment solved in 1909 episodes!\tAverage Score: 0.56\n",
      "Episode 1910\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m216.9 \u001b[0m/ 1001\n",
      "Environment solved in 1910 episodes!\tAverage Score: 0.55\n",
      "Episode 1911\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m215.0 \u001b[0m/ 1001\n",
      "Environment solved in 1911 episodes!\tAverage Score: 0.54\n",
      "Episode 1912\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m216.4 \u001b[0m/ 1001\n",
      "Environment solved in 1912 episodes!\tAverage Score: 0.55\n",
      "Episode 1913\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m211.6 \u001b[0m/ 1001\n",
      "Environment solved in 1913 episodes!\tAverage Score: 0.54\n",
      "Episode 1914\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m209.7 \u001b[0m/ 1001\n",
      "Environment solved in 1914 episodes!\tAverage Score: 0.53\n",
      "Episode 1915\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m205.2 \u001b[0m/ 1001\n",
      "Environment solved in 1915 episodes!\tAverage Score: 0.52\n",
      "Episode 1916\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m204.2 \u001b[0m/ 1001\n",
      "Environment solved in 1916 episodes!\tAverage Score: 0.52\n",
      "Episode 1917\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m203.4 \u001b[0m/ 1001\n",
      "Environment solved in 1917 episodes!\tAverage Score: 0.52\n",
      "Episode 1918\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m203.4 \u001b[0m/ 1001\n",
      "Environment solved in 1918 episodes!\tAverage Score: 0.52\n",
      "Episode 1919\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m201.5 \u001b[0m/ 1001\n",
      "Environment solved in 1919 episodes!\tAverage Score: 0.51\n",
      "Episode 1920\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m200.2 \u001b[0m/ 1001\n",
      "Environment solved in 1920 episodes!\tAverage Score: 0.51\n",
      "Episode 1921\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m199.2 \u001b[0m/ 1001\n",
      "Environment solved in 1921 episodes!\tAverage Score: 0.51\n",
      "Episode 1999\t Score Min/Mean/Max: 0.00 / \u001b[1m0.41\u001b[0m / 2.50 \t Steps Min/Mean/Max: 13 / \u001b[1m164.1 \u001b[0m/ 1001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  50% |#####################                      | ETA:  1:24:33\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2058\t Score Min/Mean/Max: 0.00 / \u001b[1m0.50\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m198.0 \u001b[0m/ 1001\n",
      "Environment solved in 2058 episodes!\tAverage Score: 0.50\n",
      "Episode 2059\t Score Min/Mean/Max: 0.00 / \u001b[1m0.50\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m198.1 \u001b[0m/ 1001\n",
      "Environment solved in 2059 episodes!\tAverage Score: 0.50\n",
      "Episode 2060\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m202.3 \u001b[0m/ 1001\n",
      "Environment solved in 2060 episodes!\tAverage Score: 0.52\n",
      "Episode 2061\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m200.4 \u001b[0m/ 1001\n",
      "Environment solved in 2061 episodes!\tAverage Score: 0.51\n",
      "Episode 2062\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m200.8 \u001b[0m/ 1001\n",
      "Environment solved in 2062 episodes!\tAverage Score: 0.51\n",
      "Episode 2063\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m201.0 \u001b[0m/ 1001\n",
      "Environment solved in 2063 episodes!\tAverage Score: 0.51\n",
      "Episode 2064\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m201.0 \u001b[0m/ 1001\n",
      "Environment solved in 2064 episodes!\tAverage Score: 0.51\n",
      "Episode 2065\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m201.3 \u001b[0m/ 1001\n",
      "Environment solved in 2065 episodes!\tAverage Score: 0.51\n",
      "Episode 2066\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m203.3 \u001b[0m/ 1001\n",
      "Environment solved in 2066 episodes!\tAverage Score: 0.52\n",
      "Episode 2067\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m203.0 \u001b[0m/ 1001\n",
      "Environment solved in 2067 episodes!\tAverage Score: 0.52\n",
      "Episode 2068\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m207.2 \u001b[0m/ 1001\n",
      "Environment solved in 2068 episodes!\tAverage Score: 0.53\n",
      "Episode 2069\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m209.4 \u001b[0m/ 1001\n",
      "Environment solved in 2069 episodes!\tAverage Score: 0.53\n",
      "Episode 2070\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m210.0 \u001b[0m/ 1001\n",
      "Environment solved in 2070 episodes!\tAverage Score: 0.54\n",
      "Episode 2071\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m214.6 \u001b[0m/ 1001\n",
      "Environment solved in 2071 episodes!\tAverage Score: 0.55\n",
      "Episode 2072\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m216.6 \u001b[0m/ 1001\n",
      "Environment solved in 2072 episodes!\tAverage Score: 0.55\n",
      "Episode 2073\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m215.8 \u001b[0m/ 1001\n",
      "Environment solved in 2073 episodes!\tAverage Score: 0.55\n",
      "Episode 2074\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m215.2 \u001b[0m/ 1001\n",
      "Environment solved in 2074 episodes!\tAverage Score: 0.55\n",
      "Episode 2075\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m215.7 \u001b[0m/ 1001\n",
      "Environment solved in 2075 episodes!\tAverage Score: 0.55\n",
      "Episode 2076\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m210.9 \u001b[0m/ 1001\n",
      "Environment solved in 2076 episodes!\tAverage Score: 0.54\n",
      "Episode 2077\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m212.6 \u001b[0m/ 1001\n",
      "Environment solved in 2077 episodes!\tAverage Score: 0.54\n",
      "Episode 2078\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m213.5 \u001b[0m/ 1001\n",
      "Environment solved in 2078 episodes!\tAverage Score: 0.54\n",
      "Episode 2079\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m215.6 \u001b[0m/ 1001\n",
      "Environment solved in 2079 episodes!\tAverage Score: 0.55\n",
      "Episode 2080\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m213.0 \u001b[0m/ 1001\n",
      "Environment solved in 2080 episodes!\tAverage Score: 0.54\n",
      "Episode 2081\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.50 \t Steps Min/Mean/Max: 14 / \u001b[1m215.3 \u001b[0m/ 1001\n",
      "Environment solved in 2081 episodes!\tAverage Score: 0.55\n",
      "Episode 2082\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.50 \t Steps Min/Mean/Max: 13 / \u001b[1m214.2 \u001b[0m/ 1001\n",
      "Environment solved in 2082 episodes!\tAverage Score: 0.54\n",
      "Episode 2083\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 1.70 \t Steps Min/Mean/Max: 13 / \u001b[1m205.0 \u001b[0m/ 655\n",
      "Environment solved in 2083 episodes!\tAverage Score: 0.52\n",
      "Episode 2084\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 1.70 \t Steps Min/Mean/Max: 13 / \u001b[1m204.8 \u001b[0m/ 655\n",
      "Environment solved in 2084 episodes!\tAverage Score: 0.52\n",
      "Episode 2085\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 1.70 \t Steps Min/Mean/Max: 13 / \u001b[1m204.3 \u001b[0m/ 655\n",
      "Environment solved in 2085 episodes!\tAverage Score: 0.52\n",
      "Episode 2086\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 1.70 \t Steps Min/Mean/Max: 13 / \u001b[1m203.7 \u001b[0m/ 655\n",
      "Environment solved in 2086 episodes!\tAverage Score: 0.52\n",
      "Episode 2087\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 1.70 \t Steps Min/Mean/Max: 13 / \u001b[1m203.9 \u001b[0m/ 655\n",
      "Environment solved in 2087 episodes!\tAverage Score: 0.52\n",
      "Episode 2088\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 1.70 \t Steps Min/Mean/Max: 13 / \u001b[1m203.9 \u001b[0m/ 655\n",
      "Environment solved in 2088 episodes!\tAverage Score: 0.52\n",
      "Episode 2089\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 1.70 \t Steps Min/Mean/Max: 13 / \u001b[1m200.2 \u001b[0m/ 655\n",
      "Environment solved in 2089 episodes!\tAverage Score: 0.51\n",
      "Episode 2090\t Score Min/Mean/Max: 0.00 / \u001b[1m0.50\u001b[0m / 1.70 \t Steps Min/Mean/Max: 13 / \u001b[1m198.2 \u001b[0m/ 655\n",
      "Environment solved in 2090 episodes!\tAverage Score: 0.50\n",
      "Episode 2091\t Score Min/Mean/Max: 0.00 / \u001b[1m0.50\u001b[0m / 1.70 \t Steps Min/Mean/Max: 13 / \u001b[1m196.6 \u001b[0m/ 655\n",
      "Environment solved in 2091 episodes!\tAverage Score: 0.50\n",
      "Episode 2099\t Score Min/Mean/Max: 0.00 / \u001b[1m0.47\u001b[0m / 1.70 \t Steps Min/Mean/Max: 13 / \u001b[1m185.8 \u001b[0m/ 655"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  52% |######################                     | ETA:  1:33:29\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2145\t Score Min/Mean/Max: 0.00 / \u001b[1m0.51\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m202.3 \u001b[0m/ 1001\n",
      "Environment solved in 2145 episodes!\tAverage Score: 0.51\n",
      "Episode 2146\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m208.9 \u001b[0m/ 1001\n",
      "Environment solved in 2146 episodes!\tAverage Score: 0.53\n",
      "Episode 2147\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m208.8 \u001b[0m/ 1001\n",
      "Environment solved in 2147 episodes!\tAverage Score: 0.53\n",
      "Episode 2148\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m208.0 \u001b[0m/ 1001\n",
      "Environment solved in 2148 episodes!\tAverage Score: 0.53\n",
      "Episode 2149\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m208.3 \u001b[0m/ 1001\n",
      "Environment solved in 2149 episodes!\tAverage Score: 0.53\n",
      "Episode 2150\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m210.1 \u001b[0m/ 1001\n",
      "Environment solved in 2150 episodes!\tAverage Score: 0.53\n",
      "Episode 2151\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m212.6 \u001b[0m/ 1001\n",
      "Environment solved in 2151 episodes!\tAverage Score: 0.54\n",
      "Episode 2152\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m210.5 \u001b[0m/ 1001\n",
      "Environment solved in 2152 episodes!\tAverage Score: 0.53\n",
      "Episode 2153\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m211.8 \u001b[0m/ 1001\n",
      "Environment solved in 2153 episodes!\tAverage Score: 0.54\n",
      "Episode 2154\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m211.3 \u001b[0m/ 1001\n",
      "Environment solved in 2154 episodes!\tAverage Score: 0.54\n",
      "Episode 2155\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m213.2 \u001b[0m/ 1001\n",
      "Environment solved in 2155 episodes!\tAverage Score: 0.54\n",
      "Episode 2156\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m211.8 \u001b[0m/ 1001\n",
      "Environment solved in 2156 episodes!\tAverage Score: 0.54\n",
      "Episode 2157\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m211.1 \u001b[0m/ 1001\n",
      "Environment solved in 2157 episodes!\tAverage Score: 0.54\n",
      "Episode 2158\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m208.6 \u001b[0m/ 1001\n",
      "Environment solved in 2158 episodes!\tAverage Score: 0.53\n",
      "Episode 2159\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m208.2 \u001b[0m/ 1001\n",
      "Environment solved in 2159 episodes!\tAverage Score: 0.53\n",
      "Episode 2160\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m203.5 \u001b[0m/ 1001\n",
      "Environment solved in 2160 episodes!\tAverage Score: 0.52\n",
      "Episode 2161\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m205.9 \u001b[0m/ 1001\n",
      "Environment solved in 2161 episodes!\tAverage Score: 0.52\n",
      "Episode 2162\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m206.7 \u001b[0m/ 1001\n",
      "Environment solved in 2162 episodes!\tAverage Score: 0.53\n",
      "Episode 2163\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m208.2 \u001b[0m/ 1001\n",
      "Environment solved in 2163 episodes!\tAverage Score: 0.53\n",
      "Episode 2164\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m210.1 \u001b[0m/ 1001\n",
      "Environment solved in 2164 episodes!\tAverage Score: 0.54\n",
      "Episode 2165\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m210.4 \u001b[0m/ 1001\n",
      "Environment solved in 2165 episodes!\tAverage Score: 0.54\n",
      "Episode 2166\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m207.3 \u001b[0m/ 1001\n",
      "Environment solved in 2166 episodes!\tAverage Score: 0.53\n",
      "Episode 2167\t Score Min/Mean/Max: 0.00 / \u001b[1m0.55\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m214.8 \u001b[0m/ 1001\n",
      "Environment solved in 2167 episodes!\tAverage Score: 0.55\n",
      "Episode 2168\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m210.7 \u001b[0m/ 1001\n",
      "Environment solved in 2168 episodes!\tAverage Score: 0.54\n",
      "Episode 2169\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m210.0 \u001b[0m/ 1001\n",
      "Environment solved in 2169 episodes!\tAverage Score: 0.54\n",
      "Episode 2170\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m209.6 \u001b[0m/ 1001\n",
      "Environment solved in 2170 episodes!\tAverage Score: 0.54\n",
      "Episode 2171\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m205.4 \u001b[0m/ 1001\n",
      "Environment solved in 2171 episodes!\tAverage Score: 0.53\n",
      "Episode 2172\t Score Min/Mean/Max: 0.00 / \u001b[1m0.52\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m203.8 \u001b[0m/ 1001\n",
      "Environment solved in 2172 episodes!\tAverage Score: 0.52\n",
      "Episode 2173\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m206.9 \u001b[0m/ 1001\n",
      "Environment solved in 2173 episodes!\tAverage Score: 0.53\n",
      "Episode 2174\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m207.4 \u001b[0m/ 1001\n",
      "Environment solved in 2174 episodes!\tAverage Score: 0.53\n",
      "Episode 2175\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m207.5 \u001b[0m/ 1001\n",
      "Environment solved in 2175 episodes!\tAverage Score: 0.53\n",
      "Episode 2176\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m207.8 \u001b[0m/ 1001\n",
      "Environment solved in 2176 episodes!\tAverage Score: 0.53\n",
      "Episode 2177\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m206.2 \u001b[0m/ 1001\n",
      "Environment solved in 2177 episodes!\tAverage Score: 0.53\n",
      "Episode 2178\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m206.0 \u001b[0m/ 1001\n",
      "Environment solved in 2178 episodes!\tAverage Score: 0.53\n",
      "Episode 2179\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m205.9 \u001b[0m/ 1001\n",
      "Environment solved in 2179 episodes!\tAverage Score: 0.53\n",
      "Episode 2180\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m208.7 \u001b[0m/ 1001\n",
      "Environment solved in 2180 episodes!\tAverage Score: 0.53\n",
      "Episode 2181\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m206.4 \u001b[0m/ 1001\n",
      "Environment solved in 2181 episodes!\tAverage Score: 0.53\n",
      "Episode 2182\t Score Min/Mean/Max: 0.00 / \u001b[1m0.53\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m208.3 \u001b[0m/ 1001\n",
      "Environment solved in 2182 episodes!\tAverage Score: 0.53\n",
      "Episode 2183\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m209.1 \u001b[0m/ 1001\n",
      "Environment solved in 2183 episodes!\tAverage Score: 0.54\n",
      "Episode 2184\t Score Min/Mean/Max: 0.00 / \u001b[1m0.54\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m211.6 \u001b[0m/ 1001\n",
      "Environment solved in 2184 episodes!\tAverage Score: 0.54\n",
      "Episode 2185\t Score Min/Mean/Max: 0.00 / \u001b[1m0.56\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m217.7 \u001b[0m/ 1001\n",
      "Environment solved in 2185 episodes!\tAverage Score: 0.56\n",
      "Episode 2186\t Score Min/Mean/Max: 0.00 / \u001b[1m0.56\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m218.2 \u001b[0m/ 1001\n",
      "Environment solved in 2186 episodes!\tAverage Score: 0.56\n",
      "Episode 2187\t Score Min/Mean/Max: 0.00 / \u001b[1m0.57\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m222.1 \u001b[0m/ 1001\n",
      "Environment solved in 2187 episodes!\tAverage Score: 0.57\n",
      "Episode 2188\t Score Min/Mean/Max: 0.00 / \u001b[1m0.57\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m222.1 \u001b[0m/ 1001\n",
      "Environment solved in 2188 episodes!\tAverage Score: 0.57\n",
      "Episode 2189\t Score Min/Mean/Max: 0.00 / \u001b[1m0.57\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m224.4 \u001b[0m/ 1001\n",
      "Environment solved in 2189 episodes!\tAverage Score: 0.57\n",
      "Episode 2190\t Score Min/Mean/Max: 0.00 / \u001b[1m0.58\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m226.7 \u001b[0m/ 1001\n",
      "Environment solved in 2190 episodes!\tAverage Score: 0.58\n",
      "Episode 2191\t Score Min/Mean/Max: 0.00 / \u001b[1m0.58\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m226.2 \u001b[0m/ 1001\n",
      "Environment solved in 2191 episodes!\tAverage Score: 0.58\n",
      "Episode 2192\t Score Min/Mean/Max: 0.00 / \u001b[1m0.58\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m228.5 \u001b[0m/ 1001\n",
      "Environment solved in 2192 episodes!\tAverage Score: 0.58\n",
      "Episode 2193\t Score Min/Mean/Max: 0.00 / \u001b[1m0.59\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m229.4 \u001b[0m/ 1001\n",
      "Environment solved in 2193 episodes!\tAverage Score: 0.59\n",
      "Episode 2194\t Score Min/Mean/Max: 0.00 / \u001b[1m0.59\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m229.5 \u001b[0m/ 1001\n",
      "Environment solved in 2194 episodes!\tAverage Score: 0.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2195\t Score Min/Mean/Max: 0.00 / \u001b[1m0.59\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m229.8 \u001b[0m/ 1001\n",
      "Environment solved in 2195 episodes!\tAverage Score: 0.59\n",
      "Episode 2196\t Score Min/Mean/Max: 0.00 / \u001b[1m0.58\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m227.2 \u001b[0m/ 1001\n",
      "Environment solved in 2196 episodes!\tAverage Score: 0.58\n",
      "Episode 2197\t Score Min/Mean/Max: 0.00 / \u001b[1m0.59\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m229.6 \u001b[0m/ 1001\n",
      "Environment solved in 2197 episodes!\tAverage Score: 0.59\n",
      "Episode 2198\t Score Min/Mean/Max: 0.00 / \u001b[1m0.61\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m238.5 \u001b[0m/ 1001\n",
      "Environment solved in 2198 episodes!\tAverage Score: 0.61\n",
      "Episode 2199\t Score Min/Mean/Max: 0.00 / \u001b[1m0.61\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m239.6 \u001b[0m/ 1001\n",
      "Environment solved in 2199 episodes!\tAverage Score: 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  55% |#######################                    | ETA:  1:46:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200\t Score Min/Mean/Max: 0.00 / \u001b[1m0.63\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m243.7 \u001b[0m/ 1001\n",
      "Environment solved in 2200 episodes!\tAverage Score: 0.63\n",
      "Episode 2201\t Score Min/Mean/Max: 0.00 / \u001b[1m0.63\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m245.8 \u001b[0m/ 1001\n",
      "Environment solved in 2201 episodes!\tAverage Score: 0.63\n",
      "Episode 2202\t Score Min/Mean/Max: 0.00 / \u001b[1m0.63\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m245.8 \u001b[0m/ 1001\n",
      "Environment solved in 2202 episodes!\tAverage Score: 0.63\n",
      "Episode 2203\t Score Min/Mean/Max: 0.00 / \u001b[1m0.65\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m252.8 \u001b[0m/ 1001\n",
      "Environment solved in 2203 episodes!\tAverage Score: 0.65\n",
      "Episode 2204\t Score Min/Mean/Max: 0.00 / \u001b[1m0.65\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m254.3 \u001b[0m/ 1001\n",
      "Environment solved in 2204 episodes!\tAverage Score: 0.65\n",
      "Episode 2205\t Score Min/Mean/Max: 0.00 / \u001b[1m0.65\u001b[0m / 2.60 \t Steps Min/Mean/Max: 14 / \u001b[1m254.1 \u001b[0m/ 1001\n",
      "Environment solved in 2205 episodes!\tAverage Score: 0.65\n",
      "Episode 2206\t Score Min/Mean/Max: 0.00 / \u001b[1m0.65\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m254.4 \u001b[0m/ 1001\n",
      "Environment solved in 2206 episodes!\tAverage Score: 0.65\n",
      "Episode 2207\t Score Min/Mean/Max: 0.00 / \u001b[1m0.65\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m253.2 \u001b[0m/ 1001\n",
      "Environment solved in 2207 episodes!\tAverage Score: 0.65\n",
      "Episode 2208\t Score Min/Mean/Max: 0.00 / \u001b[1m0.66\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m256.6 \u001b[0m/ 1001\n",
      "Environment solved in 2208 episodes!\tAverage Score: 0.66\n",
      "Episode 2209\t Score Min/Mean/Max: 0.00 / \u001b[1m0.64\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m250.8 \u001b[0m/ 1001\n",
      "Environment solved in 2209 episodes!\tAverage Score: 0.64\n",
      "Episode 2210\t Score Min/Mean/Max: 0.00 / \u001b[1m0.67\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m260.1 \u001b[0m/ 1001\n",
      "Environment solved in 2210 episodes!\tAverage Score: 0.67\n",
      "Episode 2211\t Score Min/Mean/Max: 0.00 / \u001b[1m0.68\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m264.4 \u001b[0m/ 1001\n",
      "Environment solved in 2211 episodes!\tAverage Score: 0.68\n",
      "Episode 2212\t Score Min/Mean/Max: 0.00 / \u001b[1m0.68\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m264.9 \u001b[0m/ 1001\n",
      "Environment solved in 2212 episodes!\tAverage Score: 0.68\n",
      "Episode 2213\t Score Min/Mean/Max: 0.00 / \u001b[1m0.69\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m269.3 \u001b[0m/ 1001\n",
      "Environment solved in 2213 episodes!\tAverage Score: 0.69\n",
      "Episode 2214\t Score Min/Mean/Max: 0.00 / \u001b[1m0.67\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m260.1 \u001b[0m/ 1001\n",
      "Environment solved in 2214 episodes!\tAverage Score: 0.67\n",
      "Episode 2215\t Score Min/Mean/Max: 0.00 / \u001b[1m0.67\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m260.5 \u001b[0m/ 1001\n",
      "Environment solved in 2215 episodes!\tAverage Score: 0.67\n",
      "Episode 2216\t Score Min/Mean/Max: 0.00 / \u001b[1m0.67\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m261.1 \u001b[0m/ 1001\n",
      "Environment solved in 2216 episodes!\tAverage Score: 0.67\n",
      "Episode 2217\t Score Min/Mean/Max: 0.00 / \u001b[1m0.67\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m259.6 \u001b[0m/ 1001\n",
      "Environment solved in 2217 episodes!\tAverage Score: 0.67\n",
      "Episode 2218\t Score Min/Mean/Max: 0.00 / \u001b[1m0.67\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m261.5 \u001b[0m/ 1001\n",
      "Environment solved in 2218 episodes!\tAverage Score: 0.67\n",
      "Episode 2219\t Score Min/Mean/Max: 0.00 / \u001b[1m0.68\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m263.9 \u001b[0m/ 1001\n",
      "Environment solved in 2219 episodes!\tAverage Score: 0.68\n",
      "Episode 2220\t Score Min/Mean/Max: 0.00 / \u001b[1m0.67\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m262.1 \u001b[0m/ 1001\n",
      "Environment solved in 2220 episodes!\tAverage Score: 0.67\n",
      "Episode 2221\t Score Min/Mean/Max: 0.00 / \u001b[1m0.67\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m261.9 \u001b[0m/ 1001\n",
      "Environment solved in 2221 episodes!\tAverage Score: 0.67\n",
      "Episode 2222\t Score Min/Mean/Max: 0.00 / \u001b[1m0.68\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m264.8 \u001b[0m/ 1001\n",
      "Environment solved in 2222 episodes!\tAverage Score: 0.68\n",
      "Episode 2223\t Score Min/Mean/Max: 0.00 / \u001b[1m0.68\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m266.0 \u001b[0m/ 1001\n",
      "Environment solved in 2223 episodes!\tAverage Score: 0.68\n",
      "Episode 2224\t Score Min/Mean/Max: 0.00 / \u001b[1m0.69\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m270.4 \u001b[0m/ 1001\n",
      "Environment solved in 2224 episodes!\tAverage Score: 0.69\n",
      "Episode 2225\t Score Min/Mean/Max: 0.00 / \u001b[1m0.69\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m269.8 \u001b[0m/ 1001\n",
      "Environment solved in 2225 episodes!\tAverage Score: 0.69\n",
      "Episode 2226\t Score Min/Mean/Max: 0.00 / \u001b[1m0.71\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m276.1 \u001b[0m/ 1001\n",
      "Environment solved in 2226 episodes!\tAverage Score: 0.71\n",
      "Episode 2227\t Score Min/Mean/Max: 0.00 / \u001b[1m0.71\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m275.4 \u001b[0m/ 1001\n",
      "Environment solved in 2227 episodes!\tAverage Score: 0.71\n",
      "Episode 2228\t Score Min/Mean/Max: 0.00 / \u001b[1m0.71\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m277.5 \u001b[0m/ 1001\n",
      "Environment solved in 2228 episodes!\tAverage Score: 0.71\n",
      "Episode 2229\t Score Min/Mean/Max: 0.00 / \u001b[1m0.70\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m273.3 \u001b[0m/ 1001\n",
      "Environment solved in 2229 episodes!\tAverage Score: 0.70\n",
      "Episode 2230\t Score Min/Mean/Max: 0.00 / \u001b[1m0.71\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m276.5 \u001b[0m/ 1001\n",
      "Environment solved in 2230 episodes!\tAverage Score: 0.71\n",
      "Episode 2231\t Score Min/Mean/Max: 0.00 / \u001b[1m0.70\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m274.1 \u001b[0m/ 1001\n",
      "Environment solved in 2231 episodes!\tAverage Score: 0.70\n",
      "Episode 2232\t Score Min/Mean/Max: 0.00 / \u001b[1m0.70\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m271.7 \u001b[0m/ 1001\n",
      "Environment solved in 2232 episodes!\tAverage Score: 0.70\n",
      "Episode 2233\t Score Min/Mean/Max: 0.00 / \u001b[1m0.69\u001b[0m / 2.60 \t Steps Min/Mean/Max: 15 / \u001b[1m271.5 \u001b[0m/ 1001\n",
      "Environment solved in 2233 episodes!\tAverage Score: 0.69\n",
      "Episode 2234\t Score Min/Mean/Max: 0.00 / \u001b[1m0.68\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m265.0 \u001b[0m/ 1001\n",
      "Environment solved in 2234 episodes!\tAverage Score: 0.68\n",
      "Episode 2235\t Score Min/Mean/Max: 0.00 / \u001b[1m0.67\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m263.6 \u001b[0m/ 1001\n",
      "Environment solved in 2235 episodes!\tAverage Score: 0.67\n",
      "Episode 2236\t Score Min/Mean/Max: 0.00 / \u001b[1m0.68\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m266.5 \u001b[0m/ 1001\n",
      "Environment solved in 2236 episodes!\tAverage Score: 0.68\n",
      "Episode 2237\t Score Min/Mean/Max: 0.00 / \u001b[1m0.70\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m273.4 \u001b[0m/ 1001\n",
      "Environment solved in 2237 episodes!\tAverage Score: 0.70\n",
      "Episode 2238\t Score Min/Mean/Max: 0.00 / \u001b[1m0.69\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m269.9 \u001b[0m/ 1001\n",
      "Environment solved in 2238 episodes!\tAverage Score: 0.69\n",
      "Episode 2239\t Score Min/Mean/Max: 0.00 / \u001b[1m0.71\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m276.6 \u001b[0m/ 1001\n",
      "Environment solved in 2239 episodes!\tAverage Score: 0.71\n",
      "Episode 2240\t Score Min/Mean/Max: 0.00 / \u001b[1m0.70\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m275.0 \u001b[0m/ 1001\n",
      "Environment solved in 2240 episodes!\tAverage Score: 0.70\n",
      "Episode 2241\t Score Min/Mean/Max: 0.00 / \u001b[1m0.70\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m273.3 \u001b[0m/ 1001\n",
      "Environment solved in 2241 episodes!\tAverage Score: 0.70\n",
      "Episode 2242\t Score Min/Mean/Max: 0.00 / \u001b[1m0.71\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m275.9 \u001b[0m/ 1001\n",
      "Environment solved in 2242 episodes!\tAverage Score: 0.71\n",
      "Episode 2243\t Score Min/Mean/Max: 0.00 / \u001b[1m0.71\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m276.6 \u001b[0m/ 1001\n",
      "Environment solved in 2243 episodes!\tAverage Score: 0.71\n",
      "Episode 2244\t Score Min/Mean/Max: 0.00 / \u001b[1m0.71\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m277.0 \u001b[0m/ 1001\n",
      "Environment solved in 2244 episodes!\tAverage Score: 0.71\n",
      "Episode 2245\t Score Min/Mean/Max: 0.00 / \u001b[1m0.70\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m272.3 \u001b[0m/ 1001\n",
      "Environment solved in 2245 episodes!\tAverage Score: 0.70\n",
      "Episode 2246\t Score Min/Mean/Max: 0.00 / \u001b[1m0.70\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m270.6 \u001b[0m/ 1001\n",
      "Environment solved in 2246 episodes!\tAverage Score: 0.70\n",
      "Episode 2247\t Score Min/Mean/Max: 0.00 / \u001b[1m0.72\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m279.6 \u001b[0m/ 1001\n",
      "Environment solved in 2247 episodes!\tAverage Score: 0.72\n",
      "Episode 2248\t Score Min/Mean/Max: 0.00 / \u001b[1m0.72\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m279.5 \u001b[0m/ 1001\n",
      "Environment solved in 2248 episodes!\tAverage Score: 0.72\n",
      "Episode 2249\t Score Min/Mean/Max: 0.00 / \u001b[1m0.74\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m286.8 \u001b[0m/ 1001\n",
      "Environment solved in 2249 episodes!\tAverage Score: 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2250\t Score Min/Mean/Max: 0.00 / \u001b[1m0.73\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m285.6 \u001b[0m/ 1001\n",
      "Environment solved in 2250 episodes!\tAverage Score: 0.73\n",
      "Episode 2251\t Score Min/Mean/Max: 0.00 / \u001b[1m0.73\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m284.2 \u001b[0m/ 1001\n",
      "Environment solved in 2251 episodes!\tAverage Score: 0.73\n",
      "Episode 2252\t Score Min/Mean/Max: 0.00 / \u001b[1m0.73\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m284.6 \u001b[0m/ 1001\n",
      "Environment solved in 2252 episodes!\tAverage Score: 0.73\n",
      "Episode 2253\t Score Min/Mean/Max: 0.00 / \u001b[1m0.73\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m284.5 \u001b[0m/ 1001\n",
      "Environment solved in 2253 episodes!\tAverage Score: 0.73\n",
      "Episode 2254\t Score Min/Mean/Max: 0.00 / \u001b[1m0.73\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m285.1 \u001b[0m/ 1001\n",
      "Environment solved in 2254 episodes!\tAverage Score: 0.73\n",
      "Episode 2255\t Score Min/Mean/Max: 0.00 / \u001b[1m0.74\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m290.6 \u001b[0m/ 1001\n",
      "Environment solved in 2255 episodes!\tAverage Score: 0.74\n",
      "Episode 2256\t Score Min/Mean/Max: 0.00 / \u001b[1m0.75\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m293.6 \u001b[0m/ 1001\n",
      "Environment solved in 2256 episodes!\tAverage Score: 0.75\n",
      "Episode 2257\t Score Min/Mean/Max: 0.00 / \u001b[1m0.76\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m295.7 \u001b[0m/ 1001\n",
      "Environment solved in 2257 episodes!\tAverage Score: 0.76\n",
      "Episode 2258\t Score Min/Mean/Max: 0.00 / \u001b[1m0.76\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m294.9 \u001b[0m/ 1001\n",
      "Environment solved in 2258 episodes!\tAverage Score: 0.76\n",
      "Episode 2259\t Score Min/Mean/Max: 0.00 / \u001b[1m0.76\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m295.2 \u001b[0m/ 1001\n",
      "Environment solved in 2259 episodes!\tAverage Score: 0.76\n",
      "Episode 2260\t Score Min/Mean/Max: 0.00 / \u001b[1m0.75\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m294.2 \u001b[0m/ 1001\n",
      "Environment solved in 2260 episodes!\tAverage Score: 0.75\n",
      "Episode 2261\t Score Min/Mean/Max: 0.00 / \u001b[1m0.77\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m301.0 \u001b[0m/ 1001\n",
      "Environment solved in 2261 episodes!\tAverage Score: 0.77\n",
      "Episode 2262\t Score Min/Mean/Max: 0.00 / \u001b[1m0.77\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m299.1 \u001b[0m/ 1001\n",
      "Environment solved in 2262 episodes!\tAverage Score: 0.77\n",
      "Episode 2263\t Score Min/Mean/Max: 0.00 / \u001b[1m0.76\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m297.3 \u001b[0m/ 1001\n",
      "Environment solved in 2263 episodes!\tAverage Score: 0.76\n",
      "Episode 2264\t Score Min/Mean/Max: 0.00 / \u001b[1m0.76\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m296.0 \u001b[0m/ 1001\n",
      "Environment solved in 2264 episodes!\tAverage Score: 0.76\n",
      "Episode 2265\t Score Min/Mean/Max: 0.00 / \u001b[1m0.75\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m294.6 \u001b[0m/ 1001\n",
      "Environment solved in 2265 episodes!\tAverage Score: 0.75\n",
      "Episode 2266\t Score Min/Mean/Max: 0.00 / \u001b[1m0.76\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m295.3 \u001b[0m/ 1001\n",
      "Environment solved in 2266 episodes!\tAverage Score: 0.76\n",
      "Episode 2267\t Score Min/Mean/Max: 0.00 / \u001b[1m0.74\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m289.4 \u001b[0m/ 1001\n",
      "Environment solved in 2267 episodes!\tAverage Score: 0.74\n",
      "Episode 2268\t Score Min/Mean/Max: 0.00 / \u001b[1m0.75\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m291.0 \u001b[0m/ 1001\n",
      "Environment solved in 2268 episodes!\tAverage Score: 0.75\n",
      "Episode 2269\t Score Min/Mean/Max: 0.00 / \u001b[1m0.75\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m291.2 \u001b[0m/ 1001\n",
      "Environment solved in 2269 episodes!\tAverage Score: 0.75\n",
      "Episode 2270\t Score Min/Mean/Max: 0.00 / \u001b[1m0.75\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m292.3 \u001b[0m/ 1001\n",
      "Environment solved in 2270 episodes!\tAverage Score: 0.75\n",
      "Episode 2271\t Score Min/Mean/Max: 0.00 / \u001b[1m0.75\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m294.3 \u001b[0m/ 1001\n",
      "Environment solved in 2271 episodes!\tAverage Score: 0.75\n",
      "Episode 2272\t Score Min/Mean/Max: 0.00 / \u001b[1m0.78\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m303.1 \u001b[0m/ 1001\n",
      "Environment solved in 2272 episodes!\tAverage Score: 0.78\n",
      "Episode 2273\t Score Min/Mean/Max: 0.00 / \u001b[1m0.77\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m300.3 \u001b[0m/ 1001\n",
      "Environment solved in 2273 episodes!\tAverage Score: 0.77\n",
      "Episode 2274\t Score Min/Mean/Max: 0.00 / \u001b[1m0.77\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m300.3 \u001b[0m/ 1001\n",
      "Environment solved in 2274 episodes!\tAverage Score: 0.77\n",
      "Episode 2275\t Score Min/Mean/Max: 0.00 / \u001b[1m0.77\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m299.9 \u001b[0m/ 1001\n",
      "Environment solved in 2275 episodes!\tAverage Score: 0.77\n",
      "Episode 2276\t Score Min/Mean/Max: 0.00 / \u001b[1m0.77\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m299.3 \u001b[0m/ 1001\n",
      "Environment solved in 2276 episodes!\tAverage Score: 0.77\n",
      "Episode 2277\t Score Min/Mean/Max: 0.00 / \u001b[1m0.77\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m299.5 \u001b[0m/ 1001\n",
      "Environment solved in 2277 episodes!\tAverage Score: 0.77\n",
      "Episode 2278\t Score Min/Mean/Max: 0.00 / \u001b[1m0.77\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m302.8 \u001b[0m/ 1001\n",
      "Environment solved in 2278 episodes!\tAverage Score: 0.77\n",
      "Episode 2279\t Score Min/Mean/Max: 0.00 / \u001b[1m0.78\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m304.7 \u001b[0m/ 1001\n",
      "Environment solved in 2279 episodes!\tAverage Score: 0.78\n",
      "Episode 2280\t Score Min/Mean/Max: 0.00 / \u001b[1m0.78\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m305.3 \u001b[0m/ 1001\n",
      "Environment solved in 2280 episodes!\tAverage Score: 0.78\n",
      "Episode 2281\t Score Min/Mean/Max: 0.00 / \u001b[1m0.80\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m311.0 \u001b[0m/ 1001\n",
      "Environment solved in 2281 episodes!\tAverage Score: 0.80\n",
      "Episode 2282\t Score Min/Mean/Max: 0.00 / \u001b[1m0.79\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m309.5 \u001b[0m/ 1001\n",
      "Environment solved in 2282 episodes!\tAverage Score: 0.79\n",
      "Episode 2283\t Score Min/Mean/Max: 0.00 / \u001b[1m0.81\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m317.5 \u001b[0m/ 1001\n",
      "Environment solved in 2283 episodes!\tAverage Score: 0.81\n",
      "Episode 2284\t Score Min/Mean/Max: 0.00 / \u001b[1m0.81\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m314.8 \u001b[0m/ 1001\n",
      "Environment solved in 2284 episodes!\tAverage Score: 0.81\n",
      "Episode 2285\t Score Min/Mean/Max: 0.00 / \u001b[1m0.79\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m309.5 \u001b[0m/ 1001\n",
      "Environment solved in 2285 episodes!\tAverage Score: 0.79\n",
      "Episode 2286\t Score Min/Mean/Max: 0.00 / \u001b[1m0.80\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m313.5 \u001b[0m/ 1001\n",
      "Environment solved in 2286 episodes!\tAverage Score: 0.80\n",
      "Episode 2287\t Score Min/Mean/Max: 0.00 / \u001b[1m0.82\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m319.1 \u001b[0m/ 1001\n",
      "Environment solved in 2287 episodes!\tAverage Score: 0.82\n",
      "Episode 2288\t Score Min/Mean/Max: 0.00 / \u001b[1m0.83\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m325.0 \u001b[0m/ 1001\n",
      "Environment solved in 2288 episodes!\tAverage Score: 0.83\n",
      "Episode 2289\t Score Min/Mean/Max: 0.00 / \u001b[1m0.84\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m326.3 \u001b[0m/ 1001\n",
      "Environment solved in 2289 episodes!\tAverage Score: 0.84\n",
      "Episode 2290\t Score Min/Mean/Max: 0.00 / \u001b[1m0.84\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m326.4 \u001b[0m/ 1001\n",
      "Environment solved in 2290 episodes!\tAverage Score: 0.84\n",
      "Episode 2291\t Score Min/Mean/Max: 0.00 / \u001b[1m0.84\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m327.5 \u001b[0m/ 1001\n",
      "Environment solved in 2291 episodes!\tAverage Score: 0.84\n",
      "Episode 2292\t Score Min/Mean/Max: 0.00 / \u001b[1m0.84\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m328.1 \u001b[0m/ 1001\n",
      "Environment solved in 2292 episodes!\tAverage Score: 0.84\n",
      "Episode 2293\t Score Min/Mean/Max: 0.00 / \u001b[1m0.84\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m327.5 \u001b[0m/ 1001\n",
      "Environment solved in 2293 episodes!\tAverage Score: 0.84\n",
      "Episode 2294\t Score Min/Mean/Max: 0.00 / \u001b[1m0.85\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m330.7 \u001b[0m/ 1001\n",
      "Environment solved in 2294 episodes!\tAverage Score: 0.85\n",
      "Episode 2295\t Score Min/Mean/Max: 0.00 / \u001b[1m0.87\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m337.5 \u001b[0m/ 1001\n",
      "Environment solved in 2295 episodes!\tAverage Score: 0.87\n",
      "Episode 2296\t Score Min/Mean/Max: 0.00 / \u001b[1m0.88\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m342.5 \u001b[0m/ 1001\n",
      "Environment solved in 2296 episodes!\tAverage Score: 0.88\n",
      "Episode 2297\t Score Min/Mean/Max: 0.00 / \u001b[1m0.88\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m342.1 \u001b[0m/ 1001\n",
      "Environment solved in 2297 episodes!\tAverage Score: 0.88\n",
      "Episode 2298\t Score Min/Mean/Max: 0.00 / \u001b[1m0.87\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m336.9 \u001b[0m/ 1001\n",
      "Environment solved in 2298 episodes!\tAverage Score: 0.87\n",
      "Episode 2299\t Score Min/Mean/Max: 0.00 / \u001b[1m0.86\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m334.0 \u001b[0m/ 1001\n",
      "Environment solved in 2299 episodes!\tAverage Score: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  57% |########################                   | ETA:  2:03:53\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300\t Score Min/Mean/Max: 0.00 / \u001b[1m0.87\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m339.4 \u001b[0m/ 1001\n",
      "Environment solved in 2300 episodes!\tAverage Score: 0.87\n",
      "Episode 2301\t Score Min/Mean/Max: 0.00 / \u001b[1m0.87\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m337.1 \u001b[0m/ 1001\n",
      "Environment solved in 2301 episodes!\tAverage Score: 0.87\n",
      "Episode 2302\t Score Min/Mean/Max: 0.00 / \u001b[1m0.87\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m338.9 \u001b[0m/ 1001\n",
      "Environment solved in 2302 episodes!\tAverage Score: 0.87\n",
      "Episode 2303\t Score Min/Mean/Max: 0.00 / \u001b[1m0.86\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m333.3 \u001b[0m/ 1001\n",
      "Environment solved in 2303 episodes!\tAverage Score: 0.86\n",
      "Episode 2304\t Score Min/Mean/Max: 0.00 / \u001b[1m0.87\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m338.4 \u001b[0m/ 1001\n",
      "Environment solved in 2304 episodes!\tAverage Score: 0.87\n",
      "Episode 2305\t Score Min/Mean/Max: 0.00 / \u001b[1m0.87\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m336.7 \u001b[0m/ 1001\n",
      "Environment solved in 2305 episodes!\tAverage Score: 0.87\n",
      "Episode 2306\t Score Min/Mean/Max: 0.00 / \u001b[1m0.89\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m345.3 \u001b[0m/ 1001\n",
      "Environment solved in 2306 episodes!\tAverage Score: 0.89\n",
      "Episode 2307\t Score Min/Mean/Max: 0.00 / \u001b[1m0.90\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m351.4 \u001b[0m/ 1001\n",
      "Environment solved in 2307 episodes!\tAverage Score: 0.90\n",
      "Episode 2308\t Score Min/Mean/Max: 0.00 / \u001b[1m0.91\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m353.6 \u001b[0m/ 1001\n",
      "Environment solved in 2308 episodes!\tAverage Score: 0.91\n",
      "Episode 2309\t Score Min/Mean/Max: 0.00 / \u001b[1m0.93\u001b[0m / 2.60 \t Steps Min/Mean/Max: 13 / \u001b[1m361.0 \u001b[0m/ 1001\n",
      "Environment solved in 2309 episodes!\tAverage Score: 0.93\n",
      "Episode 2310\t Score Min/Mean/Max: 0.00 / \u001b[1m0.93\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m361.0 \u001b[0m/ 1001\n",
      "Environment solved in 2310 episodes!\tAverage Score: 0.93\n",
      "Episode 2311\t Score Min/Mean/Max: 0.00 / \u001b[1m0.93\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m359.5 \u001b[0m/ 1001\n",
      "Environment solved in 2311 episodes!\tAverage Score: 0.93\n",
      "Episode 2312\t Score Min/Mean/Max: 0.00 / \u001b[1m0.93\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m358.9 \u001b[0m/ 1001\n",
      "Environment solved in 2312 episodes!\tAverage Score: 0.93\n",
      "Episode 2313\t Score Min/Mean/Max: 0.00 / \u001b[1m0.94\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m363.4 \u001b[0m/ 1001\n",
      "Environment solved in 2313 episodes!\tAverage Score: 0.94\n",
      "Episode 2314\t Score Min/Mean/Max: 0.00 / \u001b[1m0.94\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m364.9 \u001b[0m/ 1001\n",
      "Environment solved in 2314 episodes!\tAverage Score: 0.94\n",
      "Episode 2315\t Score Min/Mean/Max: 0.00 / \u001b[1m0.96\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m373.0 \u001b[0m/ 1001\n",
      "Environment solved in 2315 episodes!\tAverage Score: 0.96\n",
      "Episode 2316\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m374.2 \u001b[0m/ 1001\n",
      "Environment solved in 2316 episodes!\tAverage Score: 0.97\n",
      "Episode 2317\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m378.0 \u001b[0m/ 1001\n",
      "Environment solved in 2317 episodes!\tAverage Score: 0.98\n",
      "Episode 2318\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m378.4 \u001b[0m/ 1001\n",
      "Environment solved in 2318 episodes!\tAverage Score: 0.98\n",
      "Episode 2319\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m376.7 \u001b[0m/ 1001\n",
      "Environment solved in 2319 episodes!\tAverage Score: 0.98\n",
      "Episode 2320\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m377.9 \u001b[0m/ 1001\n",
      "Environment solved in 2320 episodes!\tAverage Score: 0.98\n",
      "Episode 2321\t Score Min/Mean/Max: 0.00 / \u001b[1m0.99\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m381.7 \u001b[0m/ 1001\n",
      "Environment solved in 2321 episodes!\tAverage Score: 0.99\n",
      "Episode 2322\t Score Min/Mean/Max: 0.00 / \u001b[1m0.99\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m380.9 \u001b[0m/ 1001\n",
      "Environment solved in 2322 episodes!\tAverage Score: 0.99\n",
      "Episode 2323\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m380.0 \u001b[0m/ 1001\n",
      "Environment solved in 2323 episodes!\tAverage Score: 0.98\n",
      "Episode 2324\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m375.9 \u001b[0m/ 1001\n",
      "Environment solved in 2324 episodes!\tAverage Score: 0.97\n",
      "Episode 2325\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m375.6 \u001b[0m/ 1001\n",
      "Environment solved in 2325 episodes!\tAverage Score: 0.97\n",
      "Episode 2326\t Score Min/Mean/Max: 0.00 / \u001b[1m0.96\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m370.4 \u001b[0m/ 1001\n",
      "Environment solved in 2326 episodes!\tAverage Score: 0.96\n",
      "Episode 2327\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m374.5 \u001b[0m/ 1001\n",
      "Environment solved in 2327 episodes!\tAverage Score: 0.97\n",
      "Episode 2328\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m376.1 \u001b[0m/ 1001\n",
      "Environment solved in 2328 episodes!\tAverage Score: 0.97\n",
      "Episode 2329\t Score Min/Mean/Max: 0.00 / \u001b[1m0.99\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m382.5 \u001b[0m/ 1001\n",
      "Environment solved in 2329 episodes!\tAverage Score: 0.99\n",
      "Episode 2330\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m375.1 \u001b[0m/ 1001\n",
      "Environment solved in 2330 episodes!\tAverage Score: 0.97\n",
      "Episode 2331\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m375.5 \u001b[0m/ 1001\n",
      "Environment solved in 2331 episodes!\tAverage Score: 0.97\n",
      "Episode 2332\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m376.2 \u001b[0m/ 1001\n",
      "Environment solved in 2332 episodes!\tAverage Score: 0.98\n",
      "Episode 2333\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m377.9 \u001b[0m/ 1001\n",
      "Environment solved in 2333 episodes!\tAverage Score: 0.98\n",
      "Episode 2334\t Score Min/Mean/Max: 0.00 / \u001b[1m0.99\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m381.8 \u001b[0m/ 1001\n",
      "Environment solved in 2334 episodes!\tAverage Score: 0.99\n",
      "Episode 2335\t Score Min/Mean/Max: 0.00 / \u001b[1m0.99\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m381.3 \u001b[0m/ 1001\n",
      "Environment solved in 2335 episodes!\tAverage Score: 0.99\n",
      "Episode 2336\t Score Min/Mean/Max: 0.00 / \u001b[1m1.00\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m384.0 \u001b[0m/ 1001\n",
      "Environment solved in 2336 episodes!\tAverage Score: 1.00\n",
      "Episode 2337\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m375.9 \u001b[0m/ 1001\n",
      "Environment solved in 2337 episodes!\tAverage Score: 0.98\n",
      "Episode 2338\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m375.2 \u001b[0m/ 1001\n",
      "Environment solved in 2338 episodes!\tAverage Score: 0.97\n",
      "Episode 2339\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m372.3 \u001b[0m/ 1001\n",
      "Environment solved in 2339 episodes!\tAverage Score: 0.97\n",
      "Episode 2340\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m374.6 \u001b[0m/ 1001\n",
      "Environment solved in 2340 episodes!\tAverage Score: 0.97\n",
      "Episode 2341\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m378.0 \u001b[0m/ 1001\n",
      "Environment solved in 2341 episodes!\tAverage Score: 0.98\n",
      "Episode 2342\t Score Min/Mean/Max: 0.00 / \u001b[1m0.99\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m379.6 \u001b[0m/ 1001\n",
      "Environment solved in 2342 episodes!\tAverage Score: 0.99\n",
      "Episode 2343\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m377.1 \u001b[0m/ 1001\n",
      "Environment solved in 2343 episodes!\tAverage Score: 0.98\n",
      "Episode 2344\t Score Min/Mean/Max: 0.00 / \u001b[1m0.99\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m380.9 \u001b[0m/ 1001\n",
      "Environment solved in 2344 episodes!\tAverage Score: 0.99\n",
      "Episode 2345\t Score Min/Mean/Max: 0.00 / \u001b[1m1.01\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m389.6 \u001b[0m/ 1001\n",
      "Environment solved in 2345 episodes!\tAverage Score: 1.01\n",
      "Episode 2346\t Score Min/Mean/Max: 0.00 / \u001b[1m0.99\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m382.0 \u001b[0m/ 1001\n",
      "Environment solved in 2346 episodes!\tAverage Score: 0.99\n",
      "Episode 2347\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m380.2 \u001b[0m/ 1001\n",
      "Environment solved in 2347 episodes!\tAverage Score: 0.98\n",
      "Episode 2348\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m380.5 \u001b[0m/ 1001\n",
      "Environment solved in 2348 episodes!\tAverage Score: 0.98\n",
      "Episode 2349\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m376.3 \u001b[0m/ 1001\n",
      "Environment solved in 2349 episodes!\tAverage Score: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2350\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m375.9 \u001b[0m/ 1001\n",
      "Environment solved in 2350 episodes!\tAverage Score: 0.97\n",
      "Episode 2351\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m378.0 \u001b[0m/ 1001\n",
      "Environment solved in 2351 episodes!\tAverage Score: 0.98\n",
      "Episode 2352\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m378.9 \u001b[0m/ 1001\n",
      "Environment solved in 2352 episodes!\tAverage Score: 0.98\n",
      "Episode 2353\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m378.8 \u001b[0m/ 1001\n",
      "Environment solved in 2353 episodes!\tAverage Score: 0.98\n",
      "Episode 2354\t Score Min/Mean/Max: 0.00 / \u001b[1m0.99\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m380.9 \u001b[0m/ 1001\n",
      "Environment solved in 2354 episodes!\tAverage Score: 0.99\n",
      "Episode 2355\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m374.7 \u001b[0m/ 1001\n",
      "Environment solved in 2355 episodes!\tAverage Score: 0.97\n",
      "Episode 2356\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m373.2 \u001b[0m/ 1001\n",
      "Environment solved in 2356 episodes!\tAverage Score: 0.97\n",
      "Episode 2357\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m376.2 \u001b[0m/ 1001\n",
      "Environment solved in 2357 episodes!\tAverage Score: 0.97\n",
      "Episode 2358\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m375.4 \u001b[0m/ 1001\n",
      "Environment solved in 2358 episodes!\tAverage Score: 0.97\n",
      "Episode 2359\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m375.9 \u001b[0m/ 1001\n",
      "Environment solved in 2359 episodes!\tAverage Score: 0.97\n",
      "Episode 2360\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m376.1 \u001b[0m/ 1001\n",
      "Environment solved in 2360 episodes!\tAverage Score: 0.97\n",
      "Episode 2361\t Score Min/Mean/Max: 0.00 / \u001b[1m0.95\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m367.0 \u001b[0m/ 1001\n",
      "Environment solved in 2361 episodes!\tAverage Score: 0.95\n",
      "Episode 2362\t Score Min/Mean/Max: 0.00 / \u001b[1m0.96\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m373.0 \u001b[0m/ 1001\n",
      "Environment solved in 2362 episodes!\tAverage Score: 0.96\n",
      "Episode 2363\t Score Min/Mean/Max: 0.00 / \u001b[1m0.96\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m370.9 \u001b[0m/ 1001\n",
      "Environment solved in 2363 episodes!\tAverage Score: 0.96\n",
      "Episode 2364\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m375.7 \u001b[0m/ 1001\n",
      "Environment solved in 2364 episodes!\tAverage Score: 0.97\n",
      "Episode 2365\t Score Min/Mean/Max: 0.00 / \u001b[1m0.97\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m374.0 \u001b[0m/ 1001\n",
      "Environment solved in 2365 episodes!\tAverage Score: 0.97\n",
      "Episode 2366\t Score Min/Mean/Max: 0.00 / \u001b[1m0.99\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m381.8 \u001b[0m/ 1001\n",
      "Environment solved in 2366 episodes!\tAverage Score: 0.99\n",
      "Episode 2367\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m379.3 \u001b[0m/ 1001\n",
      "Environment solved in 2367 episodes!\tAverage Score: 0.98\n",
      "Episode 2368\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m379.2 \u001b[0m/ 1001\n",
      "Environment solved in 2368 episodes!\tAverage Score: 0.98\n",
      "Episode 2369\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m381.8 \u001b[0m/ 1001\n",
      "Environment solved in 2369 episodes!\tAverage Score: 0.98\n",
      "Episode 2370\t Score Min/Mean/Max: 0.00 / \u001b[1m0.99\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m382.2 \u001b[0m/ 1001\n",
      "Environment solved in 2370 episodes!\tAverage Score: 0.99\n",
      "Episode 2371\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m379.9 \u001b[0m/ 1001\n",
      "Environment solved in 2371 episodes!\tAverage Score: 0.98\n",
      "Episode 2372\t Score Min/Mean/Max: 0.00 / \u001b[1m0.98\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m379.9 \u001b[0m/ 1001\n",
      "Environment solved in 2372 episodes!\tAverage Score: 0.98\n",
      "Episode 2373\t Score Min/Mean/Max: 0.00 / \u001b[1m1.00\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m386.2 \u001b[0m/ 1001\n",
      "Environment solved in 2373 episodes!\tAverage Score: 1.00\n",
      "Episode 2374\t Score Min/Mean/Max: 0.00 / \u001b[1m1.01\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m389.4 \u001b[0m/ 1001\n",
      "Environment solved in 2374 episodes!\tAverage Score: 1.01\n",
      "Episode 2375\t Score Min/Mean/Max: 0.00 / \u001b[1m1.01\u001b[0m / 2.70 \t Steps Min/Mean/Max: 13 / \u001b[1m389.5 \u001b[0m/ 1001\n",
      "Environment solved in 2375 episodes!\tAverage Score: 1.01\n"
     ]
    }
   ],
   "source": [
    "# parameters for learning\n",
    "max_t=2000          # only as start value, will be increased inside the ma_ddpg algorithm\n",
    "beta_start=1.0\n",
    "beta_end=0.01\n",
    "n_episodes=4000\n",
    "beta_episodeEnd=n_episodes\n",
    "\n",
    "# create progress bar to keep track \n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=n_episodes).start()\n",
    "\n",
    "# do actual learning\n",
    "scores = ma_ddpg(n_episodes, max_t, beta_start, beta_end, beta_episodeEnd)\n",
    "\n",
    "# save learned agents if desired (best agent is saved within training loop)\n",
    "# ut.save_agentcheckpoint(state_size, action_size, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "filename = ut.get_numberedfilename('./data/scores_MADDPG_episodes{}'.format(n_episodes, beta_episodeEnd), 'pkl')\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find save name that doesn't already exist\n",
    "filename = ut.get_numberedfilename('./data/plot_MADDPG_episodes{}'.format(n_episodes, beta_episodeEnd), 'png')\n",
    "\n",
    "# plot and save data\n",
    "scores_filtered = ut.moving_average(scores)\n",
    "plt.plot(scores, label='scores')\n",
    "plt.plot(scores_filtered, label='mean scores filtered')\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('max mean score {:.2f}'.format(np.max(scores_filtered)))\n",
    "plt.savefig(filename)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate performance over 100 episodes without exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best agents found and saved\n",
    "agents = ut.load_agents(state_size, action_size, num_agents, memory)\n",
    "\n",
    "scores = []                                # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)          # last 100 scores\n",
    "steps_window = deque(maxlen=100)           # last 100 steps per episode\n",
    "    \n",
    "for i_episode in range(1, 101):                                    # play game for 100 episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    lenEpisode = 0\n",
    "    score = np.zeros(num_agents)                       # initialize the score (for each agent)\n",
    "    steps = 0   \n",
    "    actions = [0 for i in range(num_agents)]               # list containing actions for all agents    \n",
    "    while True:\n",
    "        for i in range(num_agents):\n",
    "            actions[i] = agents[i].act(states[i], beta=0.0)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        reward = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        # do some tracking on the rewards\n",
    "        score += reward\n",
    "        steps += 1\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    scores_window.append(np.max(score))       # save most recent score\n",
    "    steps_window.append(steps)                # save number of steps\n",
    "    scores.append(np.max(score))              # save most recent score\n",
    "    ut.print_info(i_episode, scores_window, steps_window)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find save name that doesn't already exist\n",
    "filename = ut.get_numberedfilename('./data/evalplot_MADDPG_episodes{}'.format(n_episodes, beta_episodeEnd), 'png')\n",
    "\n",
    "# plot and save data\n",
    "scores_filtered = ut.moving_average(scores)\n",
    "plt.plot(scores, label='scores')\n",
    "plt.plot(scores_filtered, label='mean scores filtered')\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('mean score {:.2f}'.format(scores_filtered[-1]))\n",
    "plt.savefig(filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save agents if desired as trained agents (careful, this will overwrite the data sets used in the next section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ut.save_agenttrained(state_size, action_size, agents) # save agent networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. watch trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load trained agent if needed\n",
    "\n",
    "Note: you have to run sections 1 to 3 of the code above for initialisation of parameters. Training in section 4 can be skipped, and saved agents are loaded instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'agents' in locals():\n",
    "    print('using new trained agents')\n",
    "else:\n",
    "    print('no agents available, loading from file')\n",
    "    memory = ReplayBuffer(action_size, int(5e5), 512, 321)    # as no training in this section, just a dummy\n",
    "    agents = ut.load_trainedagents(state_size, action_size, num_agents, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    lenEpisode = 0\n",
    "    actions = [0 for i in range(num_agents)]               # list containing actions for all agents\n",
    "    actions_epi = [[] for i in range(num_agents)]         # list containing actions for all agents\n",
    "    while True:\n",
    "        for i in range(num_agents):\n",
    "            actions[i] = agents[i].act(states[i], beta=0.0)\n",
    "            actions_epi[i].append(actions[i])\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        lenEpisode += 1\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    ut.plot_actions_episode(actions_epi)\n",
    "    print('Score (max over agents) from episode {}: {:.2f}, Length of Episode is {} steps'.format(i, np.max(scores), lenEpisode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
